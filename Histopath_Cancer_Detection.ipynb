{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6684e126",
   "metadata": {},
   "source": [
    "# Histopathologic Cancer Detection\n",
    "\n",
    "This notebook follows the mini-project requirements for the Kaggle competition **Histopathologic Cancer Detection**. It contains: problem description, EDA, model building (transfer learning), training, evaluation, and instructions to create a Kaggle submission.\n",
    "\n",
    "Notes:\n",
    "- Dataset is available on Kaggle. Use the Kaggle API to download (instructions below).\n",
    "- The notebook includes placeholders and safe checks so it can be opened even when the dataset isn't yet present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b210bff",
   "metadata": {},
   "source": [
    "## 1 — Setup and dependencies\n",
    "Install Python packages and configure Kaggle credentials if you haven't already. Running the install cell in some environments (remote kernels) may be required only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e5bc0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package install (uncomment and run if needed)\n",
    "# !pip install -q -U pip\n",
    "# !pip install -q kaggle tensorflow==2.12.0 matplotlib pandas scikit-learn opencv-python scikit-image\n",
    "\n",
    "# Note: TensorFlow can be large — use an existing environment if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57fb108c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score, roc_curve, precision_recall_curve\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mtf version:\u001b[39m\u001b[33m'\u001b[39m, tf.__version__)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve\n",
    "\n",
    "print('tf version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ac8e8",
   "metadata": {},
   "source": [
    "## 2 — Data overview & EDA\n",
    "This section will inspect dataset files, show class balance and example images. The exact filenames depend on the Kaggle dataset structure (train_labels.csv and train folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1324a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('data')\n",
    "TRAIN_DIR = DATA_DIR / 'train'\n",
    "LABELS_CSV = DATA_DIR / 'train_labels.csv'\n",
    "\n",
    "print('Looking for data at:', DATA_DIR)\n",
    "print('Train dir exists?', TRAIN_DIR.exists())\n",
    "print('Labels csv exists?', LABELS_CSV.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a15a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels if available and show class balance + basic EDA\n",
    "if LABELS_CSV.exists():\n",
    "    labels = pd.read_csv(LABELS_CSV)\n",
    "    print('Labels head:')\n",
    "    print(labels.head())\n",
    "    print('\\nClass distribution:')\n",
    "    print(labels['label'].value_counts())\n",
    "\n",
    "    # Plot class balance\n",
    "    try:\n",
    "        ax = labels['label'].value_counts().sort_index().plot(kind='bar', color=['C0','C1'])\n",
    "        ax.set_xlabel('label')\n",
    "        ax.set_ylabel('count')\n",
    "        ax.set_title('Class balance')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print('Could not plot class balance:', e)\n",
    "\n",
    "    # Quick pixel statistics on a small sample (safe & fast)\n",
    "    if TRAIN_DIR.exists():\n",
    "        sample_ids = labels['id'].sample(min(200, len(labels)), random_state=42).tolist()\n",
    "        means = []\n",
    "        stds = []\n",
    "        for sid in sample_ids:\n",
    "            p = TRAIN_DIR / f'{sid}.png'\n",
    "            try:\n",
    "                im = np.array(Image.open(p).convert('RGB')) / 255.0\n",
    "                means.append(im.mean())\n",
    "                stds.append(im.std())\n",
    "            except Exception:\n",
    "                continue\n",
    "        if means:\n",
    "            print(f'Pixel mean (sample): {np.mean(means):.4f}, std: {np.mean(stds):.4f}')\n",
    "            plt.hist(means, bins=30, alpha=0.6, label='means')\n",
    "            plt.hist(stds, bins=30, alpha=0.6, label='stds')\n",
    "            plt.legend()\n",
    "            plt.title('Sample pixel means and stds')\n",
    "            plt.show()\n",
    "    else:\n",
    "        print('Train folder not available; skipping pixel stats')\n",
    "else:\n",
    "    print('Labels CSV not found. Please download dataset as described above.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3be4b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few example images (if train folder exists)\n",
    "def show_examples(n=6):\n",
    "    if not TRAIN_DIR.exists():\n",
    "        print('Train folder not found; skipping image preview')\n",
    "        return\n",
    "    img_paths = list(TRAIN_DIR.glob('*.png'))[:n]\n",
    "    if not img_paths:\n",
    "        print('No PNG images found in train folder')\n",
    "        return\n",
    "    cols = 3\n",
    "    rows = int(np.ceil(len(img_paths)/cols))\n",
    "    plt.figure(figsize=(cols*3, rows*3))\n",
    "    for i, p in enumerate(img_paths):\n",
    "        img = Image.open(p)\n",
    "        ax = plt.subplot(rows, cols, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_examples(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2faba8",
   "metadata": {},
   "source": [
    "## 3 — Data pipeline and augmentation\n",
    "We'll use TensorFlow's image dataset utilities. For large-scale experiments prefer tf.data pipelines and caching/prefetching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34424f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = (96, 96)  # original Kaggle patches are 96x96\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def make_datasets_from_labels(labels_df, train_dir, img_size=IMG_SIZE, batch_size=BATCH_SIZE, val_split=0.15, seed=123):\n",
    "    # labels_df should have columns 'id' and 'label' where 'id' is filename without extension\n",
    "    labels_df = labels_df.copy()\n",
    "    labels_df['filename'] = labels_df['id'].apply(lambda x: str(train_dir / f'{x}.png'))\n",
    "    # Create a dataset from filenames and labels\n",
    "    names = labels_df['filename'].tolist()\n",
    "    labs = labels_df['label'].astype(np.int32).tolist()\n",
    "    ds = tf.data.Dataset.from_tensor_slices((names, labs))\n",
    "\n",
    "    def _load(path, label):\n",
    "        image = tf.io.read_file(path)\n",
    "        image = tf.image.decode_png(image, channels=3)\n",
    "        image = tf.image.resize(image, img_size)\n",
    "        image = image / 255.0\n",
    "        return image, label\n",
    "\n",
    "    ds = ds.shuffle(1024, seed=seed)\n",
    "    ds = ds.map(_load, num_parallel_calls=AUTOTUNE)\n",
    "    val_count = int(len(names) * val_split)\n",
    "    val_ds = ds.take(val_count).batch(batch_size).prefetch(AUTOTUNE)\n",
    "    train_ds = ds.skip(val_count).batch(batch_size).prefetch(AUTOTUNE)\n",
    "    return train_ds, val_ds\n",
    "\n",
    "# Example: only run if labels variable exists\n",
    "if 'labels' in globals():\n",
    "    train_ds, val_ds = make_datasets_from_labels(labels, TRAIN_DIR)\n",
    "    print('train_ds and val_ds created')\n",
    "else:\n",
    "    print('labels not loaded; skip dataset creation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd477e98",
   "metadata": {},
   "source": [
    "## 4 — Model (Transfer learning)\n",
    "We'll use a small transfer-learning model (EfficientNetB0) pre-trained on ImageNet and fine-tune for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape=(*IMG_SIZE, 3), base_trainable=False):\n",
    "    base = tf.keras.applications.EfficientNetB0(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    base.trainable = base_trainable\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = tf.keras.applications.efficientnet.preprocess_input(inputs)\n",
    "    x = base(x, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0470d81",
   "metadata": {},
   "source": [
    "## 5 — Training\n",
    "Run training with callbacks (early stopping, model checkpoint). If dataset is large, train for a few epochs and monitor validation AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add39a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "if 'train_ds' in globals():\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
    "    ]\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=callbacks)\n",
    "else:\n",
    "    print('Training datasets not available — run data download and dataset creation first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d859bb",
   "metadata": {},
   "source": [
    "## 6 — Evaluation and Metrics\n",
    "Compute ROC AUC on the validation set and show curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32fe685",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'val_ds' in globals():\n",
    "    y_true = []\n",
    "    y_prob = []\n",
    "    for x_batch, y_batch in val_ds.unbatch().batch(1024):\n",
    "        preds = model.predict(x_batch, verbose=0).ravel()\n",
    "        y_prob.extend(preds.tolist())\n",
    "        y_true.extend(y_batch.numpy().tolist())\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print('Validation ROC AUC:', auc)\n",
    "else:\n",
    "    print('Validation dataset not available — skipping evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912f45b4",
   "metadata": {},
   "source": [
    "## 7 — Prepare Kaggle submission (example)\n",
    "This section shows how to run predictions on the test set and create the CSV submission required by the Kaggle competition (id, label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIR = DATA_DIR / 'test'\n",
    "SAMPLE_SUB = DATA_DIR / 'sample_submission.csv'\n",
    "# Fallback to repo root if not present under data/\n",
    "if not SAMPLE_SUB.exists():\n",
    "    SAMPLE_SUB = Path('sample_submission.csv')\n",
    "\n",
    "if TEST_DIR.exists() and SAMPLE_SUB.exists():\n",
    "    try:\n",
    "        sample = pd.read_csv(SAMPLE_SUB)\n",
    "        print('Sample submission loaded, rows =', len(sample))\n",
    "    except Exception as e:\n",
    "        print('Could not read sample submission:', e)\n",
    "    print('Test dir exists; placeholder for inference when model & test images are present')\n",
    "else:\n",
    "    print('Test folder or sample submission not found; skipping submission creation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8cd2ee",
   "metadata": {},
   "source": [
    "## 8 — Discussion & Next steps\n",
    "- Try stronger data augmentation (random rotations, color jitter).\n",
    "- Experiment with model ensembles or deeper models.\n",
    "- Use stratified k-fold CV and mixup/cutmix for improved generalization.\n",
    "- Track experiments with Weights & Biases or TensorBoard.\n",
    "\n",
    "---\n",
    "**Deliverable checklist**:\n",
    "- Notebook with EDA, model, training code, evaluation (this document).\n",
    "- `requirements.txt` provided in the repository.\n",
    "- README with basic run instructions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
